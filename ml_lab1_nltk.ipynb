{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_lab1_nltk.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMS9mOWX9eIOVP/kB5OHC80",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SabhyaGrover/ML-Coursework/blob/main/ml_lab1_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsyzRv8aHgRI"
      },
      "source": [
        "## **Assignment Lab-1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ62VjoRH7ha"
      },
      "source": [
        "**Problem Statement**:\n",
        "Suppose a user wants to find relevant documents/articles for a particular query from a given huge collection of documents. Then it becomes very difficult to search manually,as it will require a lot of efforts. So to search efficiently and automatically, we require a recommendation system that will recommend a document/article in response to a particular query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu81fpsoLatW"
      },
      "source": [
        "**EXERCISE 1: Pre-Processing Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9mNid2Od56p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbac9ee-032b-464f-a065-d8a2fb90521a"
      },
      "source": [
        "# import and download libraries from NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU2QTT8XaAqa"
      },
      "source": [
        "# Read data from Documents\n",
        "doc1=\"Broad to Rogers no run around the wicket Rogers back and across the off stump to block up the wicket\"\n",
        "doc2=\"Swann to Watson no run covers up on the off strump up the wicket\"\n",
        "doc3=\"Meth to Shahriar Nafees, no run, on a good length on the off,drives that on the up towards extra cover\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZav1yJyeIWp"
      },
      "source": [
        "# Convert to Lower Case\n",
        "doc1=doc1.lower()\n",
        "doc2=doc2.lower()\n",
        "doc3=doc3.lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w7XF0MqksOa",
        "outputId": "68fce440-1f95-4b5b-d883-cd4c44ff8231"
      },
      "source": [
        "# Remove Punctuation from each word\n",
        "import string\n",
        "doc1=doc1.translate(str.maketrans('', '', string.punctuation))\n",
        "doc2=doc2.translate(str.maketrans('', '', string.punctuation))\n",
        "doc3=doc3.translate(str.maketrans('', '', string.punctuation))\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "print(doc3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "broad to rogers no run around the wicket rogers back and across the off stump to block up the wicket\n",
            "swann to watson no run covers up on the off strump up the wicket\n",
            "meth to shahriar nafees no run on a good length on the offdrives that on the up towards extra cover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBaOaiN-gn94",
        "outputId": "54228e49-c67e-4676-a728-9d97163491fc"
      },
      "source": [
        "# Create Bag Of Words after Tokenizing each Sentence\n",
        "tokens1=nltk.word_tokenize(doc1)\n",
        "tokens2=nltk.word_tokenize(doc2)\n",
        "tokens3=nltk.word_tokenize(doc3)\n",
        "print(tokens1)\n",
        "print(tokens2)\n",
        "print(tokens3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['broad', 'to', 'rogers', 'no', 'run', 'around', 'the', 'wicket', 'rogers', 'back', 'and', 'across', 'the', 'off', 'stump', 'to', 'block', 'up', 'the', 'wicket']\n",
            "['swann', 'to', 'watson', 'no', 'run', 'covers', 'up', 'on', 'the', 'off', 'strump', 'up', 'the', 'wicket']\n",
            "['meth', 'to', 'shahriar', 'nafees', 'no', 'run', 'on', 'a', 'good', 'length', 'on', 'the', 'offdrives', 'that', 'on', 'the', 'up', 'towards', 'extra', 'cover']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV9BeIEJfQiZ",
        "outputId": "2aa37989-980d-49d7-d77e-2eaaf43e90ae"
      },
      "source": [
        "# Import Stop Words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Filter out Stop Words from the existing Tokens\n",
        "tokens1=[t for t in tokens1 if not t in stop_words]\n",
        "tokens2=[t for t in tokens2 if not t in stop_words]\n",
        "tokens3=[t for t in tokens3 if not t in stop_words]\n",
        "print(tokens1)\n",
        "print(tokens2)\n",
        "print(tokens3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['broad', 'rogers', 'run', 'around', 'wicket', 'rogers', 'back', 'across', 'stump', 'block', 'wicket']\n",
            "['swann', 'watson', 'run', 'covers', 'strump', 'wicket']\n",
            "['meth', 'shahriar', 'nafees', 'run', 'good', 'length', 'offdrives', 'towards', 'extra', 'cover']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-T84yfOplM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2p8ROjDLquD"
      },
      "source": [
        "**EXERCISE 2: Using the Pre-Processed Text for Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0lKSrJvUCqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abb6761-ebba-4ee3-f080-a76e79227ca0"
      },
      "source": [
        "# Appending all Tokens to form a Combined Corpus\n",
        "corpus = []\n",
        "corpus = set(tokens1).union(set(corpus))\n",
        "corpus = set(tokens2).union(set(corpus))\n",
        "corpus = set(tokens3).union(set(corpus))\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'back', 'wicket', 'stump', 'covers', 'watson', 'nafees', 'offdrives', 'rogers', 'broad', 'block', 'run', 'strump', 'length', 'across', 'good', 'meth', 'shahriar', 'towards', 'extra', 'cover', 'around', 'swann'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOwWRcqCzqG3"
      },
      "source": [
        "# Initiating Empty Dictionary with Key values from the Corpus\n",
        "wordDic1 = dict.fromkeys(corpus,0)\n",
        "wordDic2 = dict.fromkeys(corpus,0)\n",
        "wordDic3 = dict.fromkeys(corpus,0)\n",
        "\n",
        "# Updating Freq of Words in each Document\n",
        "for w in tokens1:\n",
        "  wordDic1[w]+=1\n",
        "\n",
        "for w in tokens2:\n",
        "  wordDic2[w]+=1\n",
        "  \n",
        "for w in tokens3:\n",
        "  wordDic3[w]+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQHEANas-Tg"
      },
      "source": [
        "# Steps to provide TF-IDF score in each documents\n",
        "\n",
        "# Step 1:Compute Term Frequency\n",
        "def computeTF(wordDict,bow):\n",
        "  tfDict = {}\n",
        "  bowCount = len(bow)\n",
        "  for word,count in wordDict.items():\n",
        "    tfDict[word] = count/float(bowCount)\n",
        "  # TF(t) = (No of times term t appears in a document)/(Total no of terms in the\n",
        "  # document).\n",
        "  return tfDict\n",
        "\n",
        "# Step 2:Compute Inverse Document Frequency\n",
        "def computeIDF(docList):\n",
        "  import math\n",
        "  idfDict = {}\n",
        "  N = len(docList)\n",
        "  idfDict = dict.fromkeys(docList[0].keys(),0)\n",
        "  for doc in docList:\n",
        "    for word,val in doc.items():\n",
        "      if val > 0:\n",
        "        idfDict[word] += 1\n",
        "  for word,val in idfDict.items():\n",
        "    idfDict[word] = math.log(N/float(val))\n",
        "  # IDF(t) = log_e(Total number of documents / Number of documents with term t\n",
        "  # in it).\n",
        "  return idfDict\n",
        "\n",
        "# Step 3: Compute the combined TF-IDF score \n",
        "def computeTFIDF(tfBow, idfs):\n",
        "  tfidf = {}\n",
        "  for word,val in tfBow.items():\n",
        "    tfidf[word] = val*idfs[word]\n",
        "  return tfidf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "NO4__pQG1unF",
        "outputId": "53fd1b7a-10fa-41bd-d576-51be9c49651b"
      },
      "source": [
        "# Step 1\n",
        "tfBow1 = computeTF(wordDic1, tokens1)\n",
        "tfBow2 = computeTF(wordDic2, tokens2)\n",
        "tfBow3 = computeTF(wordDic3, tokens3)\n",
        "# Step 2\n",
        "idfs = computeIDF([wordDic1,wordDic2,wordDic3])\n",
        "# Step 3\n",
        "tfidfBow1 = computeTFIDF(tfBow1,idfs)\n",
        "tfidfBow2 = computeTFIDF(tfBow2,idfs)\n",
        "tfidfBow3 = computeTFIDF(tfBow3,idfs)\n",
        "# Display TF-IDF score in a DataFrame\n",
        "import pandas as pd\n",
        "pd.DataFrame([tfidfBow1,tfidfBow2,tfidfBow3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>back</th>\n",
              "      <th>wicket</th>\n",
              "      <th>stump</th>\n",
              "      <th>covers</th>\n",
              "      <th>watson</th>\n",
              "      <th>nafees</th>\n",
              "      <th>offdrives</th>\n",
              "      <th>rogers</th>\n",
              "      <th>broad</th>\n",
              "      <th>block</th>\n",
              "      <th>run</th>\n",
              "      <th>strump</th>\n",
              "      <th>length</th>\n",
              "      <th>across</th>\n",
              "      <th>good</th>\n",
              "      <th>meth</th>\n",
              "      <th>shahriar</th>\n",
              "      <th>towards</th>\n",
              "      <th>extra</th>\n",
              "      <th>cover</th>\n",
              "      <th>around</th>\n",
              "      <th>swann</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.073721</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.199748</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183102</td>\n",
              "      <td>0.183102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.183102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       back    wicket     stump  ...     cover    around     swann\n",
              "0  0.099874  0.073721  0.099874  ...  0.000000  0.099874  0.000000\n",
              "1  0.000000  0.067578  0.000000  ...  0.000000  0.000000  0.183102\n",
              "2  0.000000  0.000000  0.000000  ...  0.109861  0.000000  0.000000\n",
              "\n",
              "[3 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTWKgfecOuyq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}